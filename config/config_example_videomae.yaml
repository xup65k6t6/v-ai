# Example configuration file for VideoMAE V2 training
# Note: VideoMAE V2 is experimental and may not perform as well as 3D CNN

# Dataset configuration
samples_base: "./data/videos"  # Base directory containing video samples
checkpoint_dir: "./checkpoints"  # Directory to save model checkpoints

# Video IDs for train/val/test split
# Modify these based on your dataset
train_ids: [1, 3, 6, 7, 10, 13, 15, 16, 18, 22, 23, 31, 32, 36, 38, 39, 40, 41, 42, 48, 50, 52, 53, 54]
val_ids: [0, 2, 8, 12, 17, 19, 24, 26, 27, 28, 30, 33, 46, 49, 51]
test_ids: [4, 5, 9, 11, 14, 20, 21, 25, 29, 34, 35, 37, 43, 44, 45, 47]

# Training parameters
num_epochs: 30  # Total number of training epochs
batch_size: 1  # VideoMAE requires more memory, smaller batch size recommended
learning_rate: 0.00005  # Lower learning rate for fine-tuning
patience: 10  # Early stopping patience
num_workers: 4  # Number of data loading workers
weight_decay: 0.05  # Weight decay for AdamW optimizer

# Logging configuration
wandb_project: "volleyball_group_activity"  # Weights & Biases project name
wandb_run_name: "videomae_volleyball"  # Name for this specific run

# Video processing parameters
window_before: 3  # Number of frames before the key frame
window_after: 4  # Number of frames after the key frame
num_frames: 8  # Number of frames to sample for VideoMAE
image_size: 320  # Size to resize images

# Model configuration
model_type: "videomae"  # Model architecture type
videomae_v2_model_name: "MCG-NJU/videomae-base-finetuned-kinetics"  # Pretrained model from HuggingFace
pretrained: true  # Use pretrained VideoMAE model

# Scheduler configuration
scheduler_patience: 2  # Patience for learning rate reduction

# Note: When running train_videomae_v2.py, you must specify --output_dir
# Example: python v_ai/train_videomae_v2.py --config config/config_example_videomae.yaml --output_dir ./checkpoints/videomae_run1
